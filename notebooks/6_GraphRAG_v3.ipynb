{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install requests python-dotenv neo4j pydantic langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from neo4j import GraphDatabase\n",
    "from pydantic import BaseModel, Field\n",
    "import warnings\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_request(url):\n",
    "  return requests.get(url).json()\n",
    "\n",
    "def strip_html(text):\n",
    "  \"\"\" remove HTML tags from a string \"\"\"\n",
    "  if not isinstance(text, str):\n",
    "    return \"\"\n",
    "  clean = re.compile(\"<.*?>\")\n",
    "  return re.sub(clean, \"\", text)\n",
    "\n",
    "def preprocess_events(events):\n",
    "  \"\"\" construct dictionary from event data \"\"\"\n",
    "  return [\n",
    "    {\n",
    "      \"title\": event[\"title\"],\n",
    "      \"group_title\": event[\"group_title\"],\n",
    "      \"url\": event[\"url\"],\n",
    "      \"description\": strip_html(event[\"description\"]),\n",
    "      \"date\": event[\"date\"],\n",
    "      \"date_time\": event[\"date_time\"],\n",
    "      \"location\": event[\"location\"],\n",
    "      \"location_title\": event[\"location_title\"],\n",
    "      \"location_latitude\": float(event[\"location_latitude\"]) if event[\"location_latitude\"] != None else 0,\n",
    "      \"location_longitude\": float(event[\"location_longitude\"]) if event[\"location_longitude\"] != None else 0,\n",
    "      \"cost\": event[\"cost\"],\n",
    "      \"thumbnail\": event[\"thumbnail\"],\n",
    "      \"event_types\": event[\"event_types\"],\n",
    "      \"event_types_audience\": event[\"event_types_audience\"],\n",
    "    }\n",
    "    for event in events\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_root = os.path.join(os.getcwd(), 'graphrag_index')\n",
    "os.makedirs(os.path.join(index_root, 'input'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamu_events_url = \"https://calendar.tamu.edu/live/json/events/group\"\n",
    "raw_events = get_json_request(tamu_events_url)\n",
    "processed_events = preprocess_events(raw_events)\n",
    "\n",
    "#save processed data to file\n",
    "file_path = \"inputEvents.txt\"\n",
    "with open(file_path, 'w') as file:\n",
    "    for j, event in enumerate(processed_events):\n",
    "        file.write(json.dumps(event) + \"\\n\")\n",
    "        if j == 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_docs = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        loaded_docs.append(line.strip())\n",
    "\n",
    "documents = loaded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up Neo4j database connection\n",
    "#if you change your user/password, need to restart kernel for\n",
    "#changes to take effect\n",
    "driver = GraphDatabase.driver(\n",
    "    uri=os.environ[\"NEO4J_URI\"],\n",
    "    auth=(os.environ[\"NEO4J_USERNAME\"], os.environ[\"NEO4J_PASSWORD\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fulltext_index(tx):\n",
    "    query = '''\n",
    "    CREATE FULLTEXT INDEX fulltext_entity_name \n",
    "    FOR (n:Entity) \n",
    "    ON EACH [n.name];\n",
    "    '''\n",
    "    tx.run(query)\n",
    "\n",
    "def create_index():\n",
    "    with driver.session() as session:\n",
    "        session.execute_write(create_fulltext_index)\n",
    "\n",
    "try:\n",
    "    create_index()\n",
    "except Exception as e:\n",
    "    print(\"The index already exists or there was an error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityItem(BaseModel):\n",
    "    name: str\n",
    "    type: str\n",
    "\n",
    "class Entities(BaseModel):\n",
    "    names: List[EntityItem] = Field(\n",
    "        ...,\n",
    "        description=\"List of entities with 'name' and 'type', focusing on event-related entities.\"\n",
    "    )\n",
    "\n",
    "def extract_entities(text, max_retries=3):\n",
    "    prompt_template = f\"\"\"\n",
    "    Find relevant entities in the following text, extracting \"Event\", \"Event_Type\", \n",
    "    \"Event_Types_Audience\", \"Speakers\", \"Location\", \"Department_or_Organization\", \"Topic\", and \n",
    "    \"Date\" entities. Format the output as a JSON list, where each item has 'name' and 'type' keys.\n",
    "    \n",
    "    Do not add any extra explanation or commentary, just the output specified above.\n",
    "    Create a list of entities with `name` and `type` fields, ensuring that each entity has a \n",
    "    non-null `name` value. If you can't find the `name`, do not include the entity in the response.\n",
    "\n",
    "    Text: \"{text}\"\n",
    "    \"\"\"\n",
    "\n",
    "    llm = Ollama(model=\"mistral\", temperature=0.0, num_predict=1000)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        response = llm.invoke(prompt_template)\n",
    "        print(f\"attempt {attempt + 1} response:\")\n",
    "        print(response)\n",
    "\n",
    "        try:\n",
    "            #use regex to extract JSON portion\n",
    "            json_match = re.search(r\"\\[.*\\]\", response, re.DOTALL)\n",
    "            if not json_match:\n",
    "                raise ValueError(\"no valid json array found in LLM response\")\n",
    "\n",
    "            json_data = json_match.group()\n",
    "            raw_entities = json.loads(json_data)\n",
    "\n",
    "            #filter out entities with a null name\n",
    "            valid_entities = []\n",
    "            for entity in raw_entities:\n",
    "                name = entity.get('name')\n",
    "                if name is None or (isinstance(name, list) and not all(name)):\n",
    "                    continue\n",
    "\n",
    "                if isinstance(name, list):\n",
    "                    #if name is a list, turn it into a single string\n",
    "                    entity['name'] = \", \".join(name)\n",
    "\n",
    "                if isinstance(entity['name'], str):\n",
    "                    valid_entities.append(entity)\n",
    "            \n",
    "            entities = Entities.model_validate({\"names\": valid_entities})\n",
    "            return entities\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error getting entities on attempt {attempt + 1}: {e}\")\n",
    "            time.sleep(1)\n",
    "    \n",
    "    raise ValueError(\"failed to get a valid response from LLM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO FIX: need to create an individual node for audience type faculty, staff, students, etc.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "#insert documents with extracted entities into the graph\n",
    "def add_document_to_graph(document, entities):\n",
    "    with driver.session() as session:\n",
    "        for entity in entities.names:\n",
    "            # set the Cypher query with the label based on entity type\n",
    "            #NEED TO CHANGE LATER**********\n",
    "            query = f\"\"\"\n",
    "            MERGE (e:{entity.type} {{name: $name}})\n",
    "            MERGE (d:Document {{text: $text}})\n",
    "            MERGE (d)-[:MENTIONS]->(e)\n",
    "            \"\"\"\n",
    "            \n",
    "            session.run(query, name=entity.name, text=document)\n",
    "\n",
    "#process documents\n",
    "for doc in documents:\n",
    "    try:\n",
    "        entities = extract_entities(doc)\n",
    "        # print(\"extracted entities:\", entities)\n",
    "        add_document_to_graph(doc, entities)\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping text due to LLM failure: {doc}\")\n",
    "        continue\n",
    "\n",
    "# print(\"extracted entities:\", entities)\n",
    "# print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "llm = Ollama(model=\"llama3.1\", temperature=0.0, num_predict=500)\n",
    "\n",
    "def graph_retriever(question: str):\n",
    "    result = \"\"\n",
    "    entities = extract_entities(question)  # extract entities from the question\n",
    "    \n",
    "    entity_terms = []\n",
    "    for entity in entities.names:\n",
    "        #split compound terms and add both full and partial terms\n",
    "        entity_terms.extend([\n",
    "            term.lower() for term in entity.name.split()\n",
    "        ])\n",
    "    \n",
    "    if entity_terms:\n",
    "        response = driver.session().run(\n",
    "            \"\"\"\n",
    "            MATCH (e:Event)\n",
    "            CALL {\n",
    "                WITH e\n",
    "                MATCH (e)-[:MENTIONS*1..2]-(related)\n",
    "                WHERE related.text IS NOT NULL OR related.name IS NOT NULL\n",
    "                RETURN COLLECT(DISTINCT COALESCE(related.text, related.name)) as relatedTexts\n",
    "            }\n",
    "            WITH e, relatedTexts, [text IN relatedTexts WHERE text IS NOT NULL | toLower(text)] as lowerTexts\n",
    "            WITH e, relatedTexts, lowerTexts,\n",
    "                //calculate a match score based on how many terms are found\n",
    "                size([term IN $query_terms WHERE \n",
    "                    toLower(e.name) CONTAINS term\n",
    "                    OR ANY(text IN lowerTexts WHERE text CONTAINS term)\n",
    "                ]) as matchScore\n",
    "            WHERE matchScore > 0  //at least one term found in the node\n",
    "            RETURN \n",
    "                'Event: ' + e.name + \n",
    "                '\\nMatch Score: ' + toString(matchScore) + '/' + toString(size($query_terms)) +\n",
    "                '\\nContext: ' + \n",
    "                reduce(s = \"\", text IN relatedTexts | s + \"\\n- \" + text) as output\n",
    "            ORDER BY matchScore DESC  //show best matches first\n",
    "            LIMIT 3\n",
    "            \"\"\",\n",
    "            {\"query_terms\": entity_terms}\n",
    "        )\n",
    "\n",
    "        result += \"\\n\".join([el['output'] for el in response if el['output'] is not None])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following graph data, answer the user's question within 350 words.\n",
    "    Do not mention the graph, just focus on answering the user's question.\n",
    "    \n",
    "    Graph data:\n",
    "    {result}\n",
    "    \n",
    "    Question:\n",
    "    '{question}'\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"results:\")\n",
    "    print(result)\n",
    "    \n",
    "    print(\"llm response\")\n",
    "    #generate a response using the LLM\n",
    "    llm_response = llm.invoke(prompt).strip()\n",
    "    \n",
    "    return llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph_retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#test\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgraph_retriever\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhich events are for students and researchers, but not for faculty or staff?\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#possibly implement: LLM should be able to modify Cypher query if necessary?*****\u001b[39;00m\n\u001b[1;32m      5\u001b[0m driver\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'graph_retriever' is not defined"
     ]
    }
   ],
   "source": [
    "#test\n",
    "print(graph_retriever(\"Which events are for students and researchers, but not for faculty or staff?\"))\n",
    "#possibly implement: LLM should be able to modify Cypher query if necessary?****\n",
    "\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
